import * as dotenv from "dotenv";
dotenv.config();
// keep that ^ on top
import { getAiBriefFileKey } from "@metriport/core/domain/ai-brief/generate";
import { S3Utils } from "@metriport/core/external/aws/s3";
import { executeAsynchronously } from "@metriport/core/util/concurrency";
import { out } from "@metriport/core/util/log";
import { errorToString, getEnvVarOrFail, sleep } from "@metriport/shared";
import { buildDayjs } from "@metriport/shared/common/date";
import axios, { AxiosInstance } from "axios";
import { Command } from "commander";
import dayjs from "dayjs";
import duration from "dayjs/plugin/duration";
import * as fs from "fs";
import * as readline from "readline/promises";
import { elapsedTimeAsStr, getDelayTime } from "../shared/duration";
import { initFile } from "../shared/file";
import {
  buildGetDirPathInsideNoTimestamp,
  getTimestampForFilename,
  initRunsFolder,
} from "../shared/folder";
import { getCxData } from "../shared/get-cx-data";
import { logErrorToFile } from "../shared/log";

dayjs.extend(duration);

/**
 * This script analyzes AI Brief files in S3 for patients from a CSV file and triggers
 * consolidated refresh for patients whose AI Briefs are missing or older than a threshold.
 *
 * The script:
 * 1. Reads cxId and patientId pairs from a CSV file (cxId,patientId)
 * 2. Groups them by cxId
 * 3. Checks S3 for AI Brief existence and age
 * 4. Triggers consolidated refresh for patients needing AI Brief regeneration
 *
 * The input CSV might be generated by the `sqs/peek-dlq-print-details.ts` script.
 * CSV format should be: cxId,patientId (with header row)
 *
 * Usage:
 * - Set environment variables in .env file
 * - Run: ts-node src/consolidated/bulk-refresh-ai-briefs.ts -f <csv-file-path>
 */

const aiBriefAgeThreshold = dayjs.duration(16, "hours");
const minimumDelayTime = dayjs.duration(500, "milliseconds");
const defaultDelayTime = dayjs.duration(2, "seconds");

const numberOfParallelAiBriefsAnalysis = 50;
const minJitterMillisAiBriefsAnalysis = 10;
const maxJitterMillisAiBriefsAnalysis = 100;

const numberOfParallelConsolidatedRefreshes = 30;
const minJitterMillisConsolidatedRefreshes = 5;
const maxJitterMillisConsolidatedRefreshes = 100;

const getOutputFileName = buildGetDirPathInsideNoTimestamp(
  `refresh-ai-briefs/${getTimestampForFilename()}`
);

const program = new Command();
program
  .name("bulk-refresh-ai-briefs")
  .description("CLI to refresh AI Briefs for patients from CSV file")
  .requiredOption("-f, --file <path>", "Path to CSV file with cxId,patientId pairs")
  .option(
    "-t, --threshold <hours>",
    "AI Brief age threshold in hours",
    aiBriefAgeThreshold.asHours().toString()
  )
  .option(
    "-a, --analyze-only",
    "Only analyze AI Briefs without refreshing consolidated data",
    false
  )
  .showHelpAfterError()
  .action(main)
  .parse();

type PatientRecord = {
  cxId: string;
  patientId: string;
};

type PatientToReprocess = {
  cxId: string;
  patientId: string;
  reason: "missing" | "outdated";
  lastModified?: Date;
};

async function main({
  file: csvFilePath,
  threshold: thresholdHours,
  analyzeOnly,
}: {
  file: string;
  threshold: string;
  analyzeOnly: boolean;
}) {
  await sleep(50);
  initRunsFolder();
  const { log } = out("");
  const startedAt = Date.now();
  log(`############# Starting at ${buildDayjs(startedAt).toISOString()} #############`);

  if (!csvFilePath) {
    log(">>> Error: CSV file path is required. Use -f or --file option.");
    process.exit(1);
  }

  const ageThresholdHours = parseInt(thresholdHours);
  if (isNaN(ageThresholdHours) || ageThresholdHours <= 0) {
    log(">>> Error: Threshold must be a positive number of hours.");
    process.exit(1);
  }

  // Initialize API and S3 clients
  const apiUrl = getEnvVarOrFail("API_URL");
  const aiBriefBucketName = getEnvVarOrFail("AI_BRIEF_BUCKET_NAME");
  const awsRegion = getEnvVarOrFail("AWS_REGION");

  const api = axios.create({ baseURL: apiUrl });
  const s3Utils = new S3Utils(awsRegion);

  log(`>>> Reading CSV file: ${csvFilePath}`);
  log(`>>> AI Brief age threshold: ${ageThresholdHours} hours`);
  log(`>>> Mode: ${analyzeOnly ? "ANALYZE ONLY" : "ANALYZE AND REFRESH"}`);

  // Read and parse CSV file
  const patientRecords = readCsvFile(csvFilePath);
  log(`>>> Found ${patientRecords.length} patient records`);

  // Group by cxId
  const patientsByCxId = groupPatientsByCxId(patientRecords);
  log(`>>> Grouped into ${Object.keys(patientsByCxId).length} customers`);

  // Display summary and get confirmation
  await displaySummaryAndConfirmation(patientsByCxId, ageThresholdHours, analyzeOnly, log);

  // Analyze AI Briefs in S3
  log(`>>> Analyzing AI Briefs in S3...`);
  const patientsToReprocess = await analyzeAiBriefs(
    patientsByCxId,
    ageThresholdHours,
    aiBriefBucketName,
    s3Utils,
    log
  );

  if (patientsToReprocess.length === 0) {
    log(`>>> No patients need AI Brief refresh!`);
    process.exit(0);
  }

  log(`>>> Found ${patientsToReprocess.length} patients needing AI Brief refresh`);

  // Display detailed analysis results
  displayAnalysisResults(patientsToReprocess, log);

  if (analyzeOnly) {
    log(`>>> Analysis complete. Use without --analyze-only to refresh consolidated data.`);
    process.exit(0);
  }

  // Group patients to reprocess by cxId
  const patientsToReprocessByCxId = groupPatientsByCxId(
    patientsToReprocess.map(p => ({ cxId: p.cxId, patientId: p.patientId }))
  );

  // Process each customer
  const processingResults: Array<{
    cxId: string;
    orgName: string;
    totalPatients: number;
    successCount: number;
    errorCount: number;
    errorFileName: string;
    successFileName: string;
  }> = [];

  for (const [cxId, patients] of Object.entries(patientsToReprocessByCxId)) {
    const { orgName } = await getCxData(cxId, undefined, false);
    log(`>>> Processing ${patients.length} patients for ${orgName} (${cxId})`);

    const errorFileName = getOutputFileName(`${orgName}_${cxId}`) + ".error";
    initFile(errorFileName);
    const successFileName = getOutputFileName(`${orgName}_${cxId}`) + ".success";
    initFile(successFileName);

    let ptIndex = 0;
    let successCount = 0;
    let errorCount = 0;

    await executeAsynchronously(
      patients,
      async patient => {
        const result = await refreshConsolidatedForPatient(
          patient.patientId,
          patient.cxId,
          successFileName,
          errorFileName,
          api,
          log
        );
        if (result.success) {
          successCount++;
        } else {
          errorCount++;
        }
        log(`>>> Progress: ${++ptIndex}/${patients.length} patients complete`);
        const delayTime = getDelayTime({ log, minimumDelayTime, defaultDelayTime });
        log(`...sleeping for ${delayTime} ms`);
        await sleep(delayTime);
      },
      {
        numberOfParallelExecutions: numberOfParallelConsolidatedRefreshes,
        minJitterMillis: minJitterMillisConsolidatedRefreshes,
        maxJitterMillis: maxJitterMillisConsolidatedRefreshes,
      }
    );

    processingResults.push({
      cxId,
      orgName,
      totalPatients: patients.length,
      successCount,
      errorCount,
      errorFileName,
      successFileName,
    });
  }

  // Display comprehensive summary
  displayProcessingSummary(processingResults, log);

  log(`############# Done refreshing AI Briefs in ${elapsedTimeAsStr(startedAt)} #############`);
  process.exit(0);
}

function readCsvFile(filePath: string): PatientRecord[] {
  const fileContents = fs.readFileSync(filePath, "utf-8");
  const lines = fileContents.split(/\r?\n/).filter(line => line.trim().length > 0);

  if (lines.length < 2) {
    throw new Error("CSV file must have at least a header row and one data row");
  }

  // Skip header row and parse data
  const dataLines = lines.slice(1);
  const records: PatientRecord[] = [];

  for (const line of dataLines) {
    const [cxId, patientId] = line.split(",").map(field => field.trim().replace(/['"]/g, ""));
    if (cxId && patientId) {
      records.push({ cxId, patientId });
    }
  }

  return records;
}

function groupPatientsByCxId(patients: PatientRecord[]): Record<string, PatientRecord[]> {
  const grouped: Record<string, PatientRecord[]> = {};

  for (const patient of patients) {
    if (!grouped[patient.cxId]) {
      grouped[patient.cxId] = [];
    }
    grouped[patient.cxId].push(patient);
  }

  return grouped;
}

async function displaySummaryAndConfirmation(
  patientsByCxId: Record<string, PatientRecord[]>,
  ageThresholdHours: number,
  analyzeOnly: boolean,
  log: typeof console.log
) {
  log(`\n>>> Summary:`);
  for (const [cxId, patients] of Object.entries(patientsByCxId)) {
    log(`  - ${cxId}: ${patients.length} patients`);
  }
  log(`\n>>> AI Brief age threshold: ${ageThresholdHours} hours`);
  if (analyzeOnly) {
    log(
      `\n>>> This will analyze AI Briefs in S3 to identify patients with missing or outdated AI Briefs.`
    );
    log(`>>> No consolidated data will be refreshed.`);
  } else {
    log(
      `\n>>> This will analyze AI Briefs in S3 and refresh consolidated data for patients with missing or outdated AI Briefs.`
    );
  }

  log("Are you sure you want to proceed?");
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });
  const answer = await rl.question("Type 'yes' to proceed: ");
  if (answer !== "yes") {
    log("Aborting...");
    process.exit(0);
  }
  rl.close();
}

function displayAnalysisResults(
  patientsToReprocess: PatientToReprocess[],
  log: typeof console.log
) {
  log(`\n>>> Analysis Results:`);

  // Group by reason
  const missingPatients = patientsToReprocess.filter(p => p.reason === "missing");
  const outdatedPatients = patientsToReprocess.filter(p => p.reason === "outdated");

  log(`  - Missing AI Briefs: ${missingPatients.length} patients`);
  log(`  - Outdated AI Briefs: ${outdatedPatients.length} patients`);

  if (missingPatients.length > 0) {
    log(`\n>>> Patients with missing AI Briefs:`);
    missingPatients.forEach(p => {
      log(`    - ${p.cxId}: ${p.patientId}`);
    });
  }

  if (outdatedPatients.length > 0) {
    log(`\n>>> Patients with outdated AI Briefs:`);
    outdatedPatients.forEach(p => {
      const ageHours = p.lastModified ? dayjs().diff(dayjs(p.lastModified), "hour") : "unknown";
      log(`    - ${p.cxId}: ${p.patientId} (${ageHours}h old)`);
    });
  }

  // Group by cxId for summary
  const byCxId = groupPatientsByCxId(
    patientsToReprocess.map(p => ({ cxId: p.cxId, patientId: p.patientId }))
  );
  log(`\n>>> Summary by Customer:`);
  for (const [cxId, patients] of Object.entries(byCxId)) {
    log(`  - ${cxId}: ${patients.length} patients need refresh`);
  }
}

async function analyzeAiBriefs(
  patientsByCxId: Record<string, PatientRecord[]>,
  ageThresholdHours: number,
  aiBriefBucketName: string,
  s3Utils: S3Utils,
  log: typeof console.log
): Promise<PatientToReprocess[]> {
  const allPatients = Object.values(patientsByCxId).flat();
  const patientsToReprocess: PatientToReprocess[] = [];

  await executeAsynchronously(
    allPatients,
    async patient => {
      try {
        const fileKey = getAiBriefFileKey(patient.cxId, patient.patientId);
        const fileInfo = await s3Utils.getFileInfoFromS3(fileKey, aiBriefBucketName);

        if (!fileInfo.exists) {
          patientsToReprocess.push({
            cxId: patient.cxId,
            patientId: patient.patientId,
            reason: "missing",
          });
          log(`>>> Patient ${patient.patientId}: AI Brief missing`);
          return;
        }

        // Check file age
        if (fileInfo.updatedAt) {
          const ageHours = dayjs().diff(dayjs(fileInfo.updatedAt), "hour");
          if (ageHours > ageThresholdHours) {
            patientsToReprocess.push({
              cxId: patient.cxId,
              patientId: patient.patientId,
              reason: "outdated",
              lastModified: fileInfo.updatedAt,
            });
            log(`>>> Patient ${patient.patientId}: AI Brief outdated (${ageHours}h old)`);
          } else {
            log(`>>> Patient ${patient.patientId}: AI Brief is current (${ageHours}h old)`);
          }
        }
      } catch (error) {
        log(
          `>>> Error checking AI Brief for patient ${patient.patientId}: ${errorToString(error)}`
        );
        // If we can't check, assume it needs refresh
        patientsToReprocess.push({
          cxId: patient.cxId,
          patientId: patient.patientId,
          reason: "missing",
        });
      }
    },
    {
      numberOfParallelExecutions: numberOfParallelAiBriefsAnalysis,
      minJitterMillis: minJitterMillisAiBriefsAnalysis,
      maxJitterMillis: maxJitterMillisAiBriefsAnalysis,
    }
  );

  return patientsToReprocess;
}

function displayProcessingSummary(
  processingResults: Array<{
    cxId: string;
    orgName: string;
    totalPatients: number;
    successCount: number;
    errorCount: number;
    errorFileName: string;
    successFileName: string;
  }>,
  log: typeof console.log
) {
  log(`\n>>> Processing Summary:`);

  let totalPatients = 0;
  let totalSuccesses = 0;
  let totalErrors = 0;

  for (const result of processingResults) {
    totalPatients += result.totalPatients;
    totalSuccesses += result.successCount;
    totalErrors += result.errorCount;

    log(`\n  Customer: ${result.orgName} (${result.cxId})`);
    log(`    - Total patients: ${result.totalPatients}`);
    log(`    - Successful: ${result.successCount}`);
    log(`    - Failed: ${result.errorCount}`);

    if (result.errorCount > 0) {
      log(`    - Error file: ${result.errorFileName}`);
    }
    if (result.successCount > 0) {
      log(`    - Success file: ${result.successFileName}`);
    }
  }

  log(`\n>>> Overall Summary:`);
  log(`  - Total patients processed: ${totalPatients}`);
  log(`  - Total successful: ${totalSuccesses}`);
  log(`  - Total failed: ${totalErrors}`);

  if (totalErrors > 0) {
    log(`\n>>> Files with detailed error information are available in the runs folder.`);
  }

  // Get the base folder path for the output files
  const baseFolder =
    processingResults.length > 0
      ? processingResults[0].errorFileName.replace(/\.error$/, "").replace(/\/[^/]+$/, "")
      : "runs/refresh-ai-briefs";
  log(`\n>>> Output files location: ${baseFolder}`);
}

async function refreshConsolidatedForPatient(
  patientId: string,
  cxId: string,
  successFileName: string,
  errorFileName: string,
  api: AxiosInstance,
  log: typeof console.log
): Promise<{ success: boolean; error?: string }> {
  try {
    await api.post(`/internal/patient/${patientId}/consolidated/refresh?cxId=${cxId}`);
    log(`>>> Done refresh consolidated for patient ${patientId}...`);
    fs.appendFileSync(successFileName + ".patientIds.txt", `${patientId}\n`);
    return { success: true };
  } catch (error) {
    const msg = `ERROR processing patient ${patientId}: `;
    const errorMessage = `${msg}${errorToString(error)}`;
    log(errorMessage);
    logErrorToFile(errorFileName, msg, error as Error);
    fs.appendFileSync(errorFileName + ".patientIds.txt", `${patientId}\n`);
    return { success: false, error: errorMessage };
  }
}

export default program;
